#!/usr/bin/env python3
"""
S3 Compatible Upload Script
ä½¿ç”¨ S3 å…¼å®¹ API ä¸Šä¼ æ–‡ä»¶åˆ° Backblaze B2 å­˜å‚¨
"""

import os
import sys
import time
import argparse
from pathlib import Path
from typing import List
import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from load_env import load_env
    load_env()
except ImportError:
    pass

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
ACCESS_KEY = os.getenv("S3_ACCESS_KEY", "")
SECRET_KEY = os.getenv("S3_SECRET_KEY", "")
BUCKET_NAME = os.getenv("S3_BUCKET", "")
BASE_DIR = os.getenv("S3_BASE_DIR", "collections")
ENDPOINT_URL = os.getenv("S3_ENDPOINT", "")
LOCAL_PATH = "./web"

# ç¡®ä¿ BASE_DIR ä»¥ / ç»“å°¾
if BASE_DIR and not BASE_DIR.endswith("/"):
    BASE_DIR += "/"


def create_s3_client():
    """åˆ›å»º S3 å®¢æˆ·ç«¯"""
    if not ENDPOINT_URL:
        print("âŒ Error: S3_ENDPOINT not configured")
        sys.exit(1)
    
    # ç¡®ä¿ endpoint URL åŒ…å«åè®®
    endpoint = ENDPOINT_URL if ENDPOINT_URL.startswith(('http://', 'https://')) else f"https://{ENDPOINT_URL}"
    
    return boto3.client(
        's3',
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
        endpoint_url=endpoint,
        region_name='us-east-1'  # B2 éœ€è¦ä¸€ä¸ª regionï¼Œä½†å®é™…ä¸ä½¿ç”¨
    )


def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes < 1024:
        return f"{size_bytes}B"
    elif size_bytes < 1024**2:
        return f"{size_bytes/1024:.1f}KB"
    elif size_bytes < 1024**3:
        return f"{size_bytes/1024**2:.1f}MB"
    else:
        return f"{size_bytes/1024**3:.1f}GB"


def get_local_files(local_path: Path, incremental: bool = False) -> List[Path]:
    """è·å–è¦ä¸Šä¼ çš„æœ¬åœ°æ–‡ä»¶åˆ—è¡¨"""
    marker_file = Path(".s3_last_sync")
    
    files = []
    
    if incremental:
        if marker_file.exists():
            last_sync = marker_file.stat().st_mtime
            print(f"ğŸ”„ Incremental upload - finding files newer than {time.ctime(last_sync)}")
            
            for file_path in local_path.rglob("*"):
                if file_path.is_file() and file_path.stat().st_mtime > last_sync:
                    files.append(file_path)
        else:
            print("ğŸ”„ First time incremental upload - uploading all files")
            files = [f for f in local_path.rglob("*") if f.is_file()]
    else:
        print("ğŸ“ Full upload - scanning all files")
        files = [f for f in local_path.rglob("*") if f.is_file()]
    
    return files


def upload_file(s3_client, local_path: Path, s3_key: str) -> bool:
    """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
    try:
        file_size = local_path.stat().st_size
        print(f"ğŸ“¤ Uploading: {s3_key} ({format_size(file_size)})")
        
        # ä¸Šä¼ æ–‡ä»¶
        s3_client.upload_file(
            str(local_path),
            BUCKET_NAME,
            s3_key,
            ExtraArgs={'ContentType': 'application/octet-stream'}
        )
        
        print(f"âœ… Uploaded: {s3_key}")
        return True
        
    except ClientError as e:
        print(f"âŒ Failed to upload {s3_key}: {e}")
        return False
    except Exception as e:
        print(f"âŒ Error uploading {s3_key}: {str(e)}")
        return False


def file_exists_and_same_size(s3_client, s3_key: str, local_size: int) -> bool:
    """æ£€æŸ¥è¿œç¨‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°ç›¸åŒ"""
    try:
        response = s3_client.head_object(Bucket=BUCKET_NAME, Key=s3_key)
        remote_size = response['ContentLength']
        return remote_size == local_size
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            return False
        print(f"âš ï¸ Error checking {s3_key}: {e}")
        return False


def upload_files(s3_client, local_path: Path, base_dir: str, incremental: bool = False, skip_existing: bool = True) -> None:
    """ä¸Šä¼ æ–‡ä»¶"""
    files = get_local_files(local_path, incremental)
    
    if not files:
        print("ğŸ“­ No files to upload")
        return
    
    print(f"ğŸ“Š Found {len(files)} files to upload")
    
    success_count = 0
    skipped_count = 0
    total_size = 0
    
    for i, file_path in enumerate(files, 1):
        # è®¡ç®—ç›¸å¯¹è·¯å¾„å’Œ S3 key
        relative_path = file_path.relative_to(local_path)
        s3_key = base_dir + str(relative_path).replace(os.sep, '/')
        file_size = file_path.stat().st_size
        
        # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if skip_existing and file_exists_and_same_size(s3_client, s3_key, file_size):
            print(f"â­ï¸  Skipping (same size): {s3_key}")
            skipped_count += 1
            continue
        
        if upload_file(s3_client, file_path, s3_key):
            success_count += 1
            total_size += file_size
        
        if i % 10 == 0:
            print(f"ğŸ“ˆ Progress: {i}/{len(files)} files processed")
    
    # æ›´æ–°åŒæ­¥æ ‡è®°
    if incremental or success_count > 0:
        Path(".s3_last_sync").touch()
    
    print(f"\nğŸ‰ Upload completed!")
    print(f"ğŸ“Š Summary: {success_count} uploaded, {skipped_count} skipped, {len(files)} total")
    print(f"ğŸ“ Total size: {format_size(total_size)}")


def list_remote_files(s3_client, base_dir: str) -> None:
    """åˆ—å‡ºè¿œç¨‹æ–‡ä»¶"""
    print(f"ğŸ“‹ Listing files in S3 bucket: {BUCKET_NAME}")
    
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=base_dir)
        
        print(f"{'File Name':<50} {'Size':<10} {'Modified'}")
        print("-" * 80)
        
        total_files = 0
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    total_files += 1
                    key = obj['Key']
                    size = obj['Size']
                    modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')
                    
                    # ç§»é™¤å‰ç¼€æ˜¾ç¤ºç›¸å¯¹è·¯å¾„
                    relative_key = key.replace(base_dir, "", 1)
                    
                    print(f"{relative_key:<50} {format_size(size):<10} {modified}")
        
        print(f"\nTotal files: {total_files}")
        
    except ClientError as e:
        print(f"âŒ Error listing files: {e}")


def main():
    parser = argparse.ArgumentParser(description="Upload files to S3-compatible storage (Backblaze B2)")
    parser.add_argument("local_path", nargs="?", default=None,
                       help="Local directory to upload")
    parser.add_argument("--full", action="store_true", 
                       help="Full upload (default is incremental)")
    parser.add_argument("--base-dir", "-b", type=str, default=BASE_DIR,
                       help=f"S3 base directory (default: {BASE_DIR})")
    parser.add_argument("--force", "-f", action="store_true",
                       help="Force upload even if file exists with same size")
    parser.add_argument("--list", action="store_true",
                       help="List remote files instead of uploading")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®
    if not ACCESS_KEY or not SECRET_KEY or not BUCKET_NAME:
        print("âŒ Error: Missing S3 credentials. Please set environment variables:")
        print("  S3_ACCESS_KEY")
        print("  S3_SECRET_KEY")
        print("  S3_BUCKET")
        print("  S3_ENDPOINT")
        print("\nğŸ’¡ You can create a .env file with these variables (see .env.example)")
        sys.exit(1)
    
    # æ£€æŸ¥è·¯å¾„å‚æ•°
    if args.local_path is None and not args.list:
        print("âŒ Error: Please specify a local directory to upload")
        print("\nğŸ“ Usage examples:")
        print("  python scripts/upload_to_s3.py ./web")
        print("  python scripts/upload_to_s3.py ./dist --full")
        print("  python scripts/upload_to_s3.py --list")
        sys.exit(1)
    
    if args.local_path:
        local_path = Path(args.local_path)
        if not local_path.exists():
            print(f"âŒ Error: Local path does not exist: {local_path}")
            sys.exit(1)
    
    try:
        # åˆ›å»º S3 å®¢æˆ·ç«¯
        s3_client = create_s3_client()
        
        # æµ‹è¯•è¿æ¥
        print("ğŸ” Testing S3 connection...")
        s3_client.head_bucket(Bucket=BUCKET_NAME)
        print("âœ… S3 connection successful")
        
        if args.list:
            list_remote_files(s3_client, args.base_dir)
        else:
            # ä¸Šä¼ æ–‡ä»¶ï¼ˆé»˜è®¤å¢é‡ä¸Šä¼ ï¼‰
            is_incremental = not args.full  # é»˜è®¤å¢é‡ï¼Œé™¤éæŒ‡å®š --full
            upload_files(
                s3_client, 
                local_path, 
                args.base_dir, 
                is_incremental,
                not args.force
            )
        
    except NoCredentialsError:
        print("âŒ Error: Invalid S3 credentials")
        sys.exit(1)
    except ClientError as e:
        if e.response['Error']['Code'] == 'NoSuchBucket':
            print(f"âŒ Error: Bucket '{BUCKET_NAME}' not found")
        else:
            print(f"âŒ S3 Error: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()