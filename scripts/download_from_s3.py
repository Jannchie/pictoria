#!/usr/bin/env python3
"""
S3 Compatible Download Script
ä½¿ç”¨ S3 å…¼å®¹ API ä» Backblaze B2 å­˜å‚¨ä¸‹è½½æ–‡ä»¶
"""

import os
import sys
import time
import argparse
from pathlib import Path
from typing import List, Dict
import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from load_env import load_env
    load_env()
except ImportError:
    pass

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
ACCESS_KEY = os.getenv("S3_ACCESS_KEY", "")
SECRET_KEY = os.getenv("S3_SECRET_KEY", "")
BUCKET_NAME = os.getenv("S3_BUCKET", "")
BASE_DIR = os.getenv("S3_BASE_DIR", "collections")
ENDPOINT_URL = os.getenv("S3_ENDPOINT", "")
LOCAL_PATH = "./web"

# ç¡®ä¿ BASE_DIR ä»¥ / ç»“å°¾
if BASE_DIR and not BASE_DIR.endswith("/"):
    BASE_DIR += "/"


def create_s3_client():
    """åˆ›å»º S3 å®¢æˆ·ç«¯"""
    if not ENDPOINT_URL:
        print("âŒ Error: S3_ENDPOINT not configured")
        sys.exit(1)
    
    # ç¡®ä¿ endpoint URL åŒ…å«åè®®
    endpoint = ENDPOINT_URL if ENDPOINT_URL.startswith(('http://', 'https://')) else f"https://{ENDPOINT_URL}"
    
    return boto3.client(
        's3',
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
        endpoint_url=endpoint,
        region_name='us-east-1'  # B2 éœ€è¦ä¸€ä¸ª regionï¼Œä½†å®é™…ä¸ä½¿ç”¨
    )


def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes < 1024:
        return f"{size_bytes}B"
    elif size_bytes < 1024**2:
        return f"{size_bytes/1024:.1f}KB"
    elif size_bytes < 1024**3:
        return f"{size_bytes/1024**2:.1f}MB"
    else:
        return f"{size_bytes/1024**3:.1f}GB"


def get_remote_files(s3_client, base_dir: str, incremental: bool = False) -> List[Dict]:
    """è·å–è¦ä¸‹è½½çš„è¿œç¨‹æ–‡ä»¶åˆ—è¡¨"""
    marker_file = Path(".s3_last_download")
    
    # å¢é‡ä¸‹è½½çš„æ—¶é—´æˆªæ­¢ç‚¹
    cutoff_time = None
    if incremental and marker_file.exists():
        cutoff_time = marker_file.stat().st_mtime
        print(f"ğŸ”„ Incremental download - files modified after: {time.ctime(cutoff_time)}")
    else:
        print("ğŸ“‹ Full download - downloading all files")
    
    files = []
    
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=base_dir)
        
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    # å¢é‡ä¸‹è½½æ£€æŸ¥
                    if cutoff_time:
                        obj_modified = obj['LastModified'].timestamp()
                        if obj_modified <= cutoff_time:
                            continue
                    
                    files.append({
                        'key': obj['Key'],
                        'size': obj['Size'],
                        'modified': obj['LastModified']
                    })
    
    except ClientError as e:
        print(f"âŒ Error listing files: {e}")
        return []
    
    return files


def download_file(s3_client, s3_key: str, local_path: Path, file_size: int) -> bool:
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å¤§å°ç›¸åŒ
        if local_path.exists() and local_path.stat().st_size == file_size:
            print(f"â­ï¸  Skipping (same size): {local_path.name}")
            return True
        
        # åˆ›å»ºæœ¬åœ°ç›®å½•
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        relative_path = s3_key.replace(BASE_DIR, "", 1)
        print(f"ğŸ“¥ Downloading: {relative_path} ({format_size(file_size)})")
        
        # ä¸‹è½½æ–‡ä»¶
        s3_client.download_file(BUCKET_NAME, s3_key, str(local_path))
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        downloaded_size = local_path.stat().st_size
        if downloaded_size == file_size:
            print(f"âœ… Downloaded: {relative_path}")
            return True
        else:
            print(f"âš ï¸  Size mismatch for {relative_path}: expected {file_size}, got {downloaded_size}")
            local_path.unlink()
            return False
            
    except ClientError as e:
        print(f"âŒ Error downloading {s3_key}: {e}")
        if local_path.exists():
            local_path.unlink()
        return False
    except Exception as e:
        print(f"âŒ Error downloading {s3_key}: {str(e)}")
        if local_path.exists():
            local_path.unlink()
        return False


def download_files(s3_client, local_path: Path, base_dir: str, incremental: bool = False) -> None:
    """ä¸‹è½½æ–‡ä»¶"""
    print(f"ğŸ“ Starting download to: {local_path}")
    local_path.mkdir(parents=True, exist_ok=True)
    
    files = get_remote_files(s3_client, base_dir, incremental)
    
    if not files:
        print("ğŸ“­ No files to download")
        return
    
    print(f"ğŸ“Š Found {len(files)} files to download")
    
    success_count = 0
    skipped_count = 0
    total_size = 0
    
    for i, file_info in enumerate(files, 1):
        s3_key = file_info['key']
        file_size = file_info['size']
        
        # è®¡ç®—æœ¬åœ°è·¯å¾„
        relative_path = s3_key.replace(base_dir, "", 1) if base_dir else s3_key
        local_file_path = local_path / relative_path
        
        if download_file(s3_client, s3_key, local_file_path, file_size):
            if local_file_path.exists() and local_file_path.stat().st_size == file_size:
                success_count += 1
                total_size += file_size
            else:
                skipped_count += 1
        
        # æ˜¾ç¤ºè¿›åº¦
        if i % 10 == 0:
            print(f"ğŸ“ˆ Progress: {i}/{len(files)} files processed")
    
    # æ›´æ–°ä¸‹è½½æ ‡è®°
    if incremental or success_count > 0:
        Path(".s3_last_download").write_text(str(int(time.time())))
    
    print(f"\nğŸ‰ Download completed!")
    print(f"ğŸ“Š Summary: {success_count} downloaded, {skipped_count} skipped, {len(files)} total")
    print(f"ğŸ“ Total size: {format_size(total_size)}")


def list_remote_files(s3_client, base_dir: str) -> None:
    """åˆ—å‡ºè¿œç¨‹æ–‡ä»¶"""
    print(f"ğŸ“‹ Listing files in S3 bucket: {BUCKET_NAME}")
    
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=base_dir)
        
        print(f"{'File Name':<50} {'Size':<10} {'Modified'}")
        print("-" * 80)
        
        total_files = 0
        total_size = 0
        
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    total_files += 1
                    key = obj['Key']
                    size = obj['Size']
                    modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')
                    total_size += size
                    
                    # ç§»é™¤å‰ç¼€æ˜¾ç¤ºç›¸å¯¹è·¯å¾„
                    relative_key = key.replace(base_dir, "", 1)
                    
                    print(f"{relative_key:<50} {format_size(size):<10} {modified}")
        
        print(f"\nTotal files: {total_files}")
        print(f"Total size: {format_size(total_size)}")
        
    except ClientError as e:
        print(f"âŒ Error listing files: {e}")


def sync_from_remote(s3_client, local_path: Path, base_dir: str) -> None:
    """ä»è¿œç¨‹åŒæ­¥ï¼Œåˆ é™¤æœ¬åœ°å¤šä½™çš„æ–‡ä»¶"""
    print("ğŸ”„ Syncing from remote (will delete local files not in remote)...")
    
    # è·å–è¿œç¨‹æ–‡ä»¶åˆ—è¡¨
    remote_files = set()
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=base_dir)
        
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    key = obj['Key']
                    relative_key = key.replace(base_dir, "", 1)
                    remote_files.add(relative_key)
    except ClientError as e:
        print(f"âŒ Error listing remote files: {e}")
        return
    
    # æŸ¥æ‰¾æœ¬åœ°å¤šä½™çš„æ–‡ä»¶
    local_files = set()
    if local_path.exists():
        for file_path in local_path.rglob("*"):
            if file_path.is_file():
                relative_path = str(file_path.relative_to(local_path)).replace(os.sep, '/')
                local_files.add(relative_path)
    
    # åˆ é™¤æœ¬åœ°å¤šä½™çš„æ–‡ä»¶
    extra_files = local_files - remote_files
    if extra_files:
        print(f"ğŸ—‘ï¸ Deleting {len(extra_files)} local files not in remote:")
        for relative_path in extra_files:
            local_file = local_path / relative_path
            if local_file.exists():
                local_file.unlink()
                print(f"  Deleted: {relative_path}")
    
    # ä¸‹è½½è¿œç¨‹æ–‡ä»¶
    download_files(s3_client, local_path, base_dir, incremental=False)


def main():
    parser = argparse.ArgumentParser(description="Download files from S3-compatible storage (Backblaze B2)")
    parser.add_argument("local_path", nargs="?", default=None,
                       help="Local directory to download to")
    parser.add_argument("command", nargs="?", default="incremental", 
                       choices=["download", "incremental", "list", "sync"],
                       help="Command to execute (default: incremental)")
    parser.add_argument("--base-dir", "-b", type=str, default=BASE_DIR,
                       help=f"S3 base directory (default: {BASE_DIR})")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®
    if not ACCESS_KEY or not SECRET_KEY or not BUCKET_NAME:
        print("âŒ Error: Missing S3 credentials. Please set environment variables:")
        print("  S3_ACCESS_KEY")
        print("  S3_SECRET_KEY")
        print("  S3_BUCKET")
        print("  S3_ENDPOINT")
        print("\nğŸ’¡ You can create a .env file with these variables (see .env.example)")
        sys.exit(1)
    
    # æ£€æŸ¥è·¯å¾„å‚æ•°
    if args.local_path is None:
        print("âŒ Error: Please specify a local directory for download")
        print("\nğŸ“ Usage examples:")
        print("  python scripts/download_from_s3.py ./web")
        print("  python scripts/download_from_s3.py ./backup download")
        print("  python scripts/download_from_s3.py ./web list")
        print("  python scripts/download_from_s3.py ./backup sync")
        sys.exit(1)
    
    local_path = Path(args.local_path)
    
    try:
        # åˆ›å»º S3 å®¢æˆ·ç«¯
        s3_client = create_s3_client()
        
        # æµ‹è¯•è¿æ¥
        print("ğŸ” Testing S3 connection...")
        s3_client.head_bucket(Bucket=BUCKET_NAME)
        print("âœ… S3 connection successful")
        
        # æ‰§è¡Œå‘½ä»¤
        if args.command == "download":
            download_files(s3_client, local_path, args.base_dir, incremental=False)
        elif args.command == "incremental":
            download_files(s3_client, local_path, args.base_dir, incremental=True)
        elif args.command == "list":
            list_remote_files(s3_client, args.base_dir)
        elif args.command == "sync":
            sync_from_remote(s3_client, local_path, args.base_dir)
        
    except NoCredentialsError:
        print("âŒ Error: Invalid S3 credentials")
        sys.exit(1)
    except ClientError as e:
        if e.response['Error']['Code'] == 'NoSuchBucket':
            print(f"âŒ Error: Bucket '{BUCKET_NAME}' not found")
        else:
            print(f"âŒ S3 Error: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()